<?xml version="1.0" encoding="UTF-8"?>
<xml><records><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Nguyen, Tung Thanh</author><author>Nguyen, Anh Tuan</author><author>Nguyen, Hoan Anh</author><author>Nguyen, Tien N.</author></authors></contributors><titles><title>A statistical semantic language model for source code</title><secondary-title>the 2013 9th Joint Meeting</secondary-title></titles><periodical><full-title>Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering - ESEC/FSE 2013</full-title></periodical><pages>532</pages><dates><year>2013</year><pub-dates><date>2013</date></pub-dates></dates><publisher>ACM Press</publisher><isbn>978-1-4503-2237-9</isbn><electronic-resource-num>10.1145/2491411.2491458</electronic-resource-num><abstract>Recent research has successfully applied the statistical ngram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/ functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18–68% higher accuracy than the state-of-the-art approach.</abstract><remote-database-name>Crossref</remote-database-name><language>en</language><urls><web-urls><url>http://dl.acm.org/citation.cfm?doid=2491411.2491458</url></web-urls><pdf-urls><url>internal-pdf://55/Nguyen 等。 - 2013 - A statistical semantic language model for source c.pdf</url></pdf-urls></urls><access-date>2019-03-11 16:37:00</access-date><custom1>Saint Petersburg, Russia</custom1><custom3>Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering - ESEC/FSE 2013</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Ray, Baishakhi</author><author>Hellendoorn, Vincent</author><author>Godhane, Saheel</author><author>Tu, Zhaopeng</author><author>Bacchelli, Alberto</author><author>Devanbu, Premkumar</author></authors></contributors><titles><title>On the "Naturalness" of Buggy Code</title><secondary-title>arXiv:1506.01159 [cs]</secondary-title></titles><periodical><full-title>arXiv:1506.01159 [cs]</full-title></periodical><keywords><keyword>68N30</keyword><keyword>Computer Science - Software Engineering</keyword></keywords><dates><year>2015</year><pub-dates><date>2015-06-03</date></pub-dates></dates><abstract>Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be "natural", like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is "unnatural" in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca.~8,296), from 10 different Java projects, and we focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e., unnatural), becoming less so as bugs are fixed. Focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid language-independent and simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1506.01159</url></web-urls><pdf-urls><url>internal-pdf://59/Ray 等。 - 2015 - On the Naturalness of Buggy Code.pdf</url></pdf-urls><text-urls><url>internal-pdf://60/1506.html</url></text-urls></urls><access-date>2019-03-11 17:09:08</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Allamanis, Miltiadis</author><author>Sutton, Charles</author></authors></contributors><titles><title>Mining source code repositories at massive scale using language modeling</title><secondary-title>2013 10th IEEE Working Conference on Mining Software Repositories (MSR 2013)</secondary-title></titles><periodical><full-title>2013 10th Working Conference on Mining Software Repositories (MSR)</full-title></periodical><pages>207-216</pages><dates><year>2013</year><pub-dates><date>05/2013</date></pub-dates></dates><publisher>IEEE</publisher><isbn>978-1-4673-2936-1 978-1-4799-0345-0</isbn><electronic-resource-num>10.1109/MSR.2013.6624029</electronic-resource-num><abstract>The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by ﬁnding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the ﬁrst giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is signiﬁcantly better at the code suggestion task than previous models. More broadly, our approach provides a new “lens” for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program’s core logic based solely on general information theoretic criteria.</abstract><remote-database-name>Crossref</remote-database-name><language>en</language><urls><web-urls><url>http://ieeexplore.ieee.org/document/6624029/</url></web-urls><pdf-urls><url>internal-pdf://61/Allamanis 和 Sutton - 2013 - Mining source code repositories at massive scale u.pdf</url></pdf-urls></urls><access-date>2019-03-11 17:10:59</access-date><custom1>San Francisco, CA, USA</custom1><custom3>2013 10th Working Conference on Mining Software Repositories (MSR)</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Iyer, Srinivasan</author><author>Konstas, Ioannis</author><author>Cheung, Alvin</author><author>Zettlemoyer, Luke</author></authors></contributors><titles><title>Summarizing Source Code using a Neural Attention Model</title></titles><periodical><full-title>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</full-title></periodical><pages>2073–2083</pages><dates><year>2016</year><pub-dates><date>August 2016</date></pub-dates></dates><publisher>Association for Computational Linguistics</publisher><electronic-resource-num>10.18653/v1/P16-1195</electronic-resource-num><remote-database-name>ACLWeb</remote-database-name><urls><web-urls><url>http://www.aclweb.org/anthology/P16-1195</url></web-urls><pdf-urls><url>internal-pdf://64/Iyer 等。 - 2016 - Summarizing Source Code using a Neural Attention M.pdf</url></pdf-urls></urls><access-date>2019-03-12 07:17:17</access-date><custom3>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Dam, Hoa Khanh</author><author>Tran, Truyen</author><author>Pham, Trang</author></authors></contributors><titles><title>A deep language model for software code</title><secondary-title>arXiv:1608.02715 [cs, stat]</secondary-title></titles><periodical><full-title>arXiv:1608.02715 [cs, stat]</full-title></periodical><keywords><keyword>Computer Science - Software Engineering</keyword><keyword>Statistics - Machine Learning</keyword></keywords><dates><year>2016</year><pub-dates><date>2016-08-09</date></pub-dates></dates><abstract>Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1608.02715</url></web-urls><pdf-urls><url>internal-pdf://66/Dam 等。 - 2016 - A deep language model for software code.pdf</url></pdf-urls><text-urls><url>internal-pdf://67/1608.html</url></text-urls></urls><access-date>2019-03-12 07:19:21</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Hellendoorn, Vincent J.</author><author>Devanbu, Premkumar</author></authors></contributors><titles><title>Are deep neural networks the best choice for modeling source code?</title><secondary-title>the 2017 11th Joint Meeting</secondary-title></titles><periodical><full-title>Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering  - ESEC/FSE 2017</full-title></periodical><pages>763-773</pages><dates><year>2017</year><pub-dates><date>2017</date></pub-dates></dates><publisher>ACM Press</publisher><isbn>978-1-4503-5105-8</isbn><electronic-resource-num>10.1145/3106237.3106290</electronic-resource-num><abstract>Current statistical language modeling techniques, including deeplearning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add &amp; remove text, and mix &amp; swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N -gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N -gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models.</abstract><remote-database-name>Crossref</remote-database-name><language>en</language><urls><web-urls><url>http://dl.acm.org/citation.cfm?doid=3106237.3106290</url></web-urls><pdf-urls><url>internal-pdf://68/Hellendoorn 和 Devanbu - 2017 - Are deep neural networks the best choice for model.pdf</url></pdf-urls></urls><access-date>2019-03-12 07:22:52</access-date><custom1>Paderborn, Germany</custom1><custom3>Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering  - ESEC/FSE 2017</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Web Page">12</ref-type><titles><title>Supervised Deep Features for Software Functional Clone Detection by Exploiting Lexical and Syntactical Information in Source Cod</title></titles><dates/><urls><web-urls><url>https://webcache.googleusercontent.com/search?q=cache:6ZzXbVTT3DUJ:https://cs.nju.edu.cn/lim/publications/ijcai17-clone.pdf+&amp;cd=1&amp;hl=ja&amp;ct=clnk&amp;gl=uk</url></web-urls><text-urls><url>internal-pdf://71/search.html</url></text-urls></urls><access-date>2019-03-12 08:08:04</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Tu, Zhaopeng</author><author>Su, Zhendong</author><author>Devanbu, Premkumar</author></authors></contributors><titles><title>On the Localness of Software</title></titles><pages>12</pages><dates/><abstract>The n-gram language model, which has its roots in statistical natural language processing, has been shown to successfully capture the repetitive and predictable regularities (“naturalness") of source code, and help with tasks such as code suggestion, porting, and designing assistive coding devices. However, we show in this paper that this natural-language-based model fails to exploit a special property of source code: localness. We ﬁnd that human-written programs are localized: they have useful local regularities that can be captured and exploited. We introduce a novel cache language model that consists of both an n-gram and an added “cache" component to exploit localness. We show empirically that the additional cache component greatly improves the n-gram approach by capturing the localness of software, as measured by both cross-entropy and suggestion accuracy. Our model’s suggestion accuracy is actually comparable to a state-of-the-art, semantically augmented language model; but it is simpler and easier to implement. Our cache language model requires nothing beyond lexicalization, and thus is applicable to all programming languages.</abstract><remote-database-name>Zotero</remote-database-name><language>en</language><urls><pdf-urls><url>internal-pdf://92/Tu 等。 - On the Localness of Software.pdf</url></pdf-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hindle, Abram</author><author>Barr, Earl</author><author>Gabel, Mark</author><author>Su, Zhendong</author><author>Devanbu, Prem</author></authors></contributors><titles><title>On the Naturalness of Software</title></titles><pages>12</pages><dates/><abstract>Natural languages like English are rich, complex, what people actually write or say. In the 1980’s, a fundamental and powerful. The highly creative and graceful use of languages shift to corpus-based, statistically rigorous methods occurred. like English and Tamil, by masters like Shakespeare and Avvai- The availability of large, on-line corpora of natural language yar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension.</abstract><remote-database-name>Zotero</remote-database-name><language>en</language><urls><pdf-urls><url>internal-pdf://94/Hindle 等。 - On the Naturalness of Software.pdf</url></pdf-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>White, M.</author><author>Vendome, C.</author><author>Linares-Vasquez, M.</author><author>Poshyvanyk, D.</author></authors></contributors><titles><title>Toward Deep Learning Software Repositories</title><secondary-title>2015 IEEE/ACM 12th Working Conference on Mining Software Repositories</secondary-title></titles><periodical><full-title>2015 IEEE/ACM 12th Working Conference on Mining Software Repositories</full-title></periodical><pages>334-345</pages><keywords><keyword>automatic compositional representation learning</keyword><keyword>code suggestion</keyword><keyword>Computational modeling</keyword><keyword>Computer architecture</keyword><keyword>connectionist models</keyword><keyword>Context</keyword><keyword>Context modeling</keyword><keyword>deep learning</keyword><keyword>deep-learning software repositories</keyword><keyword>high-quality models</keyword><keyword>hyperparameters</keyword><keyword>Java</keyword><keyword>Java project corpus</keyword><keyword>learning (artificial intelligence)</keyword><keyword>lexically analyzed source code files</keyword><keyword>machine learning</keyword><keyword>Machine learning</keyword><keyword>n-grams</keyword><keyword>natural language processing</keyword><keyword>neural networks</keyword><keyword>NLP techniques</keyword><keyword>program testing</keyword><keyword>programming language</keyword><keyword>project management</keyword><keyword>SE community</keyword><keyword>SE task</keyword><keyword>sequential data model</keyword><keyword>Software</keyword><keyword>software artifact conceptualization</keyword><keyword>software corpora</keyword><keyword>software engineering community</keyword><keyword>software language modeling</keyword><keyword>software language models</keyword><keyword>software lexicon improvement</keyword><keyword>Software repositories</keyword><keyword>software token streaming</keyword><keyword>source code (software)</keyword><keyword>Training</keyword></keywords><dates><year>2015</year><pub-dates><date>May 2015</date></pub-dates></dates><electronic-resource-num>10.1109/MSR.2015.38</electronic-resource-num><abstract>Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.</abstract><remote-database-name>IEEE Xplore</remote-database-name><urls><pdf-urls><url>internal-pdf://98/White 等。 - 2015 - Toward Deep Learning Software Repositories.pdf</url></pdf-urls><text-urls><url>internal-pdf://99/7180092.html</url></text-urls></urls><custom3>2015 IEEE/ACM 12th Working Conference on Mining Software Repositories</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Karampatsis, Rafael-Michael</author><author>Sutton, Charles</author></authors></contributors><titles><title>Maybe Deep Neural Networks are the Best Choice for Modeling Source Code</title><secondary-title>arXiv:1903.05734 [cs]</secondary-title></titles><periodical><full-title>arXiv:1903.05734 [cs]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Computer Science - Software Engineering</keyword></keywords><dates><year>2019</year><pub-dates><date>2019-03-13</date></pub-dates></dates><abstract>Statistical language modeling techniques have successfully been applied to source code, yielding a variety of new software development tools, such as tools for code suggestion and improving readability. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. But traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance. But the open vocabulary version of the neural network language models for code have not been introduced in the literature. We present a new open-vocabulary neural language model for code that is not limited to a fixed vocabulary of identifier names. We employ a segmentation into subword units, subsequences of tokens chosen based on a compression criterion, following previous work in machine translation. Our network achieves best in class performance, outperforming even the state-of-the-art methods of Hellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to a new test project, resulting in increased performance. We showcase our methodology on code corpora in three different languages of over a billion tokens each, hundreds of times larger than in previous work. To our knowledge, this is the largest neural language model for code that has been reported.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1903.05734</url></web-urls><pdf-urls><url>internal-pdf://101/Karampatsis 和 Sutton - 2019 - Maybe Deep Neural Networks are the Best Choice for.pdf</url></pdf-urls><text-urls><url>internal-pdf://102/1903.html</url></text-urls></urls><access-date>2019-05-07 18:13:16</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Santos, Eddie A.</author><author>Campbell, Joshua Charles</author><author>Hindle, Abram</author><author>Amaral, José Nelson</author></authors></contributors><titles><title>Finding and correcting syntax errors using recurrent neural networks</title><secondary-title>PeerJ PrePrints</secondary-title></titles><periodical><full-title>PeerJ PrePrints</full-title></periodical><pages>e3123</pages><volume>5</volume><keywords><keyword>Artificial neural network</keyword><keyword>JavaScript</keyword><keyword>Language model</keyword><keyword>Long short-term memory</keyword><keyword>LR parser</keyword><keyword>Network model</keyword><keyword>Parsing</keyword><keyword>Programmer</keyword><keyword>Recurrent neural network</keyword><keyword>Syntax error</keyword></keywords><dates><year>2017</year><pub-dates><date>2017</date></pub-dates></dates><electronic-resource-num>10.7287/peerj.preprints.3123v1</electronic-resource-num><abstract>Minor syntax errors are made by novice and experienced programmers alike; however, novice programmers lack the years of intuition that help them resolve these tiny errors. Standard LR parsers typically resolve syntax errors and their precise location poorly. We propose a methodology that helps locate where syntax errors occur, but also suggests possible changes to the token stream that can fix the error identified. This methodology finds syntax errors by checking if two language models “agree” on each token. If the models disagree, it indicates a possible syntax error; the methodology tries to suggest a fix by finding an alternative token sequence obtained from the models. We trained two LSTM (Long short-term memory) language models on a large corpus of JavaScript code collected from GitHub. The dual LSTM neural network model predicts the correct location of the syntax error 54.74% in its top 4 suggestions and produces an exact fix up to 35.50% of the time. The results show that this tool and methodology can locate and suggest corrections for syntax errors. Our methodology is of practical use to all programmers, but will be especially useful to novices frustrated with incomprehensible syntax errors.</abstract><remote-database-name>Semantic Scholar</remote-database-name><urls><pdf-urls><url>internal-pdf://106/Santos 等。 - 2017 - Finding and correcting syntax errors using recurre.pdf</url></pdf-urls><text-urls><url>https://www.semanticscholar.org/paper/Finding-and-correcting-syntax-errors-using-neural-Santos-Campbell/1c92e44a13b31541e40c478363636e9a3ce17ee9</url></text-urls></urls></record></records></xml>